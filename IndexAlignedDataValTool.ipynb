{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Aligned Data Comparison Tool\n",
    "This notebook starts with a demonstration using fake data described in the sibling README file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module Imports and configuration\n",
    "\"\"\"\n",
    "# Data Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from pprint import pprint\n",
    "\n",
    "# config\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 1 and 2 and 3\n",
    "1. Load Data - Read data from source, in this case csv files, into 2 or more dataframes\n",
    "2. Ensure the dataframes share the same column names\n",
    "3. Add each of the dataframes to a dictionary with an appropriate name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define a few constants'''\n",
    "# The name of the index column in your data\n",
    "indexColName = 'patient_id'\n",
    "# Name of the columns that will be present in both original dataframes you wish to compare\n",
    "compareCols = ['cv','coag','hep','nerv','renal','resp']\n",
    "\n",
    "# define paths to example csv files\n",
    "goldStandardFilePath = 'exampledata/goldStandard.csv'\n",
    "computedFilePath = 'exampledata/computed.csv'\n",
    "\n",
    "# use Pandas to Load the files\n",
    "goldStandardDf = pd.read_csv(goldStandardFilePath)\n",
    "computedDf = pd.read_csv(computedFilePath)\n",
    "\n",
    "# drop rows with null values\n",
    "goldStandardDf.dropna(inplace=True)\n",
    "computedDf.dropna(inplace=True)\n",
    "\n",
    "# set indexes\n",
    "goldStandardDf.set_index(indexColName, inplace=True)\n",
    "computedDf.set_index(indexColName, inplace=True)\n",
    "\n",
    "# Columns - Here we filter both tables down to the columns we wish to compare\n",
    "# NOTE: Your incoming dataframes must have the columns in compareCols.  If the load process\n",
    "# loaded them with different names, you will need to add a renaming step prior to this step\n",
    "goldStandardDf = goldStandardDf.filter(compareCols)\n",
    "computedDf = computedDf.filter(compareCols)\n",
    "\n",
    "# final step add your dataframes to a dictionary\n",
    "# This allows us to keep track of their content with a named key\n",
    "dfDict = {\n",
    "    'Gold Standard': goldStandardDf,\n",
    "    'Computed':computedDf\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the loaded data\n",
    "When reviewing the data check for:\n",
    "1. column names are the same\n",
    "1. data types are the same\n",
    "\n",
    "If either of these are different, add a few lines of code in the load process to resolve that now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review the gold standard data\n",
    "print(goldStandardDf.info())\n",
    "goldStandardDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(computedDf.info())\n",
    "computedDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Index Alignment\n",
    "Here we make sure that both dataframes, computedDf and goldStandardDf contain the same records by making sure the indexes in all of the dataframes in our dfDict only contain records in all the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignIndexes(dfDict):\n",
    "    '''\n",
    "    Takes list of N dataframes that are already indexed with the same values\n",
    "    Returns a list N dataframes containing only the indicies found in the intersection of\n",
    "    all index values\n",
    "    '''\n",
    "    indexSetList = [set(df.index.to_list()) for df in dfDict.values()]\n",
    "    commonIndexSet = set.intersection(*indexSetList)\n",
    "    print('There are {} intersecting index values'.format(len(commonIndexSet)))\n",
    "    dfRetDict = {}\n",
    "    for k,v in dfDict.items():\n",
    "        print('Data Source = {}'.format(k))\n",
    "        print('{} entries before index alignment'.format(len(v)))\n",
    "        dfAligned = v.loc[list(commonIndexSet)]\n",
    "        print('{} entries after index alignment'.format(len(dfAligned)))\n",
    "        dfRetDict[k] = dfAligned\n",
    "    return dfRetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAlignedDict = alignIndexes(dfDict)\n",
    "pprint(dfAlignedDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Merge the index aligned dataframes together\n",
    "When the frames are merged, the column names will overlap so pandas will add a 'grouping layer' to the column index that ensures the data from each frame is faithfully represented in the final frame.  The grouping layer names come from the names assigned to each frame in step 3.  This allows us to track the provenance of all data in the merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - glue dataframes together\n",
    "# Glues the computed dataframe on to the right edge of the gold standard dataframe\n",
    "# adds a level to the column index which acts as a grouper\n",
    "# this denotes which data was from which source\n",
    "df_all = pd.concat(dfAlignedDict.values(),\n",
    "                   keys=dfAlignedDict.keys(),\n",
    "                   axis='columns')\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange the columns pairwise\n",
    "Purely for asthetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Interleave columns\n",
    "# By swapping the column name grouping levels we can arrange each subscores data value with it's sibling source\n",
    "df_final = df_all.swaplevel(axis='columns')[sorted(set(x[1] for x in df_all.columns))]\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Compare the Data: Visual Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function is the primary key to visually grepping the data differences and obtaining a short report\n",
    "about how many (absolute count/relative proportion) of differences there are\n",
    "'''\n",
    "def styleDifferences(df, groupAssumedCorrect=None, color='yellow'):\n",
    "    '''\n",
    "    Function that consumes an index aligned dataframe in which each column has a sibling\n",
    "    from a different 'group'\n",
    "    This will do pair wise directional comparisons from one sibling to another, highlighting\n",
    "    discordances with the color of your choosing\n",
    "    arguments:\n",
    "    1. indexAlignedDf\n",
    "    2. groupAssumedCorrect - group assumed to contain ground truth.\n",
    "    '''\n",
    "    style = 'background-color: {}'.format(color)\n",
    "    # sets up a cross section of the dataframe to compare against\n",
    "    # This carves out the values from the group (computed|goldstandard you wish to treat as ground truth)\n",
    "    # level = -1 carves out the innermost level of the column grouping levels\n",
    "    goldStandard = df.xs(groupAssumedCorrect, axis='columns', level=-1)\n",
    "    # builds a new dataframe with cell highlighting\n",
    "    # DataFrame constructor is pd.DataFrame(data, index, column_names)\n",
    "    # conditionally applied when pairwise columnar values are not equal\n",
    "    # pandas ne() function is tailor made for this operation\n",
    "    styledDataframe = pd.DataFrame(np.where(\n",
    "                                        df.ne(goldStandard, axis='columns', level=0),\n",
    "                                        style,  # apply style when goldStandard cross section != dataframe value\n",
    "                                        ''), # Don't apply style when the values are equal\n",
    "                                   index=df.index, \n",
    "                                   columns=df.columns)\n",
    "    return styledDataframe\n",
    "\n",
    "def computePercentDiscordance(df, groupAssumedCorrect=None):\n",
    "    '''\n",
    "    Computes the percent discordance between groud truth group values\n",
    "    and the alternative group\n",
    "    '''\n",
    "    goldStandard = df.xs(groupAssumedCorrect, axis='columns', level=-1)\n",
    "    return round((df[df.ne(goldStandard, axis='columns', level=0)].count()/df.count())*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply the diff \n",
    "df_final.style.apply(styleDifferences, axis=None, groupAssumedCorrect='Gold Standard', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Compare the Data: Percent Discordance\n",
    "Rough metric to summarize how much discordance and in which columns for frames that are too large to visualize easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computePercentDiscordance(df_final, groupAssumedCorrect='Gold Standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Patterns found in visual diffs\n",
    "When the discordance is fairly sparse, it can be assumed that the discordances are either because one of the reviewers is wrong, data was not avialable to one of the reviewers, or it was an edged case and judgment or defaults were used instead.\n",
    "\n",
    "However, when the visual discordances have a pattern to them they get more interesting and usually easier to solve!  Here we present two patterns we see commonly:\n",
    "1. Systematic error in a metric - columnar disagreement\n",
    "1. Systematic error in a record - index disagreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Metric Error'''\n",
    "'''Define a few constants'''\n",
    "# The name of the index column in your data\n",
    "indexColName = 'patient_id'\n",
    "# Name of the columns that will be present in both original dataframes you wish to compare\n",
    "compareCols = ['cv','coag','hep','nerv','renal','resp']\n",
    "\n",
    "# define paths to example csv files\n",
    "goldStandardFilePath = 'exampledata/goldStandardMetric.csv'\n",
    "computedFilePath = 'exampledata/computedMetric.csv'\n",
    "\n",
    "# use Pandas to Load the files\n",
    "goldStandardDf = pd.read_csv(goldStandardFilePath)\n",
    "computedDf = pd.read_csv(computedFilePath)\n",
    "\n",
    "# drop rows with null values\n",
    "goldStandardDf.dropna(inplace=True)\n",
    "computedDf.dropna(inplace=True)\n",
    "\n",
    "# set indexes\n",
    "goldStandardDf.set_index(indexColName, inplace=True)\n",
    "computedDf.set_index(indexColName, inplace=True)\n",
    "\n",
    "# Columns - Here we filter both tables down to the columns we wish to compare\n",
    "# NOTE: Your incoming dataframes must have the columns in compareCols.  If the load process\n",
    "# loaded them with different names, you will need to add a renaming step prior to this step\n",
    "goldStandardDf = goldStandardDf.filter(compareCols)\n",
    "computedDf = computedDf.filter(compareCols)\n",
    "\n",
    "# final step add your dataframes to a dictionary\n",
    "# This allows us to keep track of their content with a named key\n",
    "dfDict = {\n",
    "    'Gold Standard': goldStandardDf,\n",
    "    'Computed':computedDf\n",
    "}\n",
    "\n",
    "dfAlignedDict = alignIndexes(dfDict)\n",
    "pprint(dfAlignedDict)\n",
    "\n",
    "# Step 2 - glue dataframes together\n",
    "# Glues the computed dataframe on to the right edge of the gold standard dataframe\n",
    "# adds a level to the column index which acts as a grouper\n",
    "# this denotes which data was from which source\n",
    "df_all = pd.concat(dfAlignedDict.values(),\n",
    "                   keys=dfAlignedDict.keys(),\n",
    "                   axis='columns')\n",
    "\n",
    "df_final = df_all.swaplevel(axis='columns')[sorted(set(x[1] for x in df_all.columns))]\n",
    "\n",
    "# Apply the diff \n",
    "df_final.style.apply(styleDifferences, axis=None, groupAssumedCorrect='Gold Standard', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Disagreement Summary\n",
    "Here we can clearly see there was a systemic disagreement on the Cardiovascular score between the expert labelers and the machine.  Often, finding the single difference leads to finding concordance for the entire set of differences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Record Error'''\n",
    "'''Define a few constants'''\n",
    "# The name of the index column in your data\n",
    "indexColName = 'patient_id'\n",
    "# Name of the columns that will be present in both original dataframes you wish to compare\n",
    "compareCols = ['cv','coag','hep','nerv','renal','resp']\n",
    "\n",
    "# define paths to example csv files\n",
    "goldStandardFilePath = 'exampledata/goldStandardRecord.csv'\n",
    "computedFilePath = 'exampledata/computedRecord.csv'\n",
    "\n",
    "# use Pandas to Load the files\n",
    "goldStandardDf = pd.read_csv(goldStandardFilePath)\n",
    "computedDf = pd.read_csv(computedFilePath)\n",
    "\n",
    "# drop rows with null values\n",
    "goldStandardDf.dropna(inplace=True)\n",
    "computedDf.dropna(inplace=True)\n",
    "\n",
    "# set indexes\n",
    "goldStandardDf.set_index(indexColName, inplace=True)\n",
    "computedDf.set_index(indexColName, inplace=True)\n",
    "\n",
    "# Columns - Here we filter both tables down to the columns we wish to compare\n",
    "# NOTE: Your incoming dataframes must have the columns in compareCols.  If the load process\n",
    "# loaded them with different names, you will need to add a renaming step prior to this step\n",
    "goldStandardDf = goldStandardDf.filter(compareCols)\n",
    "computedDf = computedDf.filter(compareCols)\n",
    "\n",
    "# final step add your dataframes to a dictionary\n",
    "# This allows us to keep track of their content with a named key\n",
    "dfDict = {\n",
    "    'Gold Standard': goldStandardDf,\n",
    "    'Computed':computedDf\n",
    "}\n",
    "\n",
    "dfAlignedDict = alignIndexes(dfDict)\n",
    "pprint(dfAlignedDict)\n",
    "\n",
    "# Step 2 - glue dataframes together\n",
    "# Glues the computed dataframe on to the right edge of the gold standard dataframe\n",
    "# adds a level to the column index which acts as a grouper\n",
    "# this denotes which data was from which source\n",
    "df_all = pd.concat(dfAlignedDict.values(),\n",
    "                   keys=dfAlignedDict.keys(),\n",
    "                   axis='columns')\n",
    "\n",
    "df_final = df_all.swaplevel(axis='columns')[sorted(set(x[1] for x in df_all.columns))]\n",
    "\n",
    "# Apply the diff \n",
    "df_final.style.apply(styleDifferences, axis=None, groupAssumedCorrect='Gold Standard', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Disagreement Summary\n",
    "Here, it is clearly seen that record number 4 gave the machine algorithm a problem.  Identifying the root cause of this issue typically brings all scores back into agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A - Interrater Agreement\n",
    "The example provided above relies on having a true Gold Standard dataset for comparative derivation.  To establish a ground truth it is useful to obtain input from several expert humans and adjudicate that input.  Common approaches typically involve an odd number of human annotators in which one of the human annotators is named the 'leading expert' and has the final say in the case of disagreements.  Using the same process as above we can visualize disagreements.  It is also helpful to employ an inter-rater agreement metric as part of this process when quantified agreement statistics are necessary.  Included here are examples of both.  The steps are very similar to those above:\n",
    "1. Load Data\n",
    "1. Align Indicies and columns\n",
    "1. Compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human1FilePath = 'exampledata/human1.csv'\n",
    "human1Df = pd.read_csv(human1FilePath)\n",
    "human1Df.dropna(inplace=True)\n",
    "human1Df.set_index(indexColName, inplace=True)\n",
    "\n",
    "human2FilePath = 'exampledata/human2.csv'\n",
    "human2Df = pd.read_csv(human2FilePath)\n",
    "human2Df.dropna(inplace=True)\n",
    "human2Df.set_index(indexColName, inplace=True)\n",
    "\n",
    "human3FilePath = 'exampledata/human3.csv'\n",
    "human3Df = pd.read_csv(human3FilePath)\n",
    "human3Df.dropna(inplace=True)\n",
    "human3Df.set_index(indexColName, inplace=True)\n",
    "\n",
    "dfDict = {\n",
    "    'Human1': human1Df,\n",
    "    'Human2': human2Df,\n",
    "    'Human3': human3Df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAlignedDict = alignIndexes(dfDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interrater Visual Diff\n",
    "Same styleDifferences() function can compare more than 2 sources of data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat(dfAlignedDict.values(),\n",
    "                   keys=dfAlignedDict.keys(),\n",
    "                   axis='columns')\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Interleave columns\n",
    "# By swapping the column name grouping levels we can arrange each subscores data value with it's sibling source\n",
    "df_final = df_all.swaplevel(axis='columns')[sorted(set(x[1] for x in df_all.columns))]\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the diff \n",
    "df_final.style.apply(styleDifferences, axis=None, groupAssumedCorrect='Human1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interrater Statistics\n",
    "NOTE: This code takes advantage of NLTK's nicely designed AnnotationTask function.  You must install nltk for this example to work:\n",
    "```py\n",
    "pipenv install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.metrics.agreement import AnnotationTask\n",
    "# using NLTK's agreement function\n",
    "def createAgreementData(dfDict):\n",
    "    '''\n",
    "    Need to create 3 tuples in the form\n",
    "    (coder, item, value)\n",
    "    In this case, we get \n",
    "    - coder from the key name in our dictionary\n",
    "    - item by concatenating the index value and column name\n",
    "    - value by extracting the value from the dataframe (value of key in our dictionary)\n",
    "    '''\n",
    "    data = []\n",
    "    for k,v in dfDict.items():\n",
    "        for row in v.itertuples():\n",
    "            idx=getattr(row,'Index')\n",
    "            for col in compareCols:\n",
    "                val = getattr(row, col)\n",
    "                data.append((k,col+str(idx),val))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreementData = createAgreementData(dfAlignedDict)\n",
    "agreementStats = AnnotationTask(agreementData)\n",
    "# Krippendorff\n",
    "print('Krippendorffs Alpha = {}'.format(agreementStats.alpha()))\n",
    "# Cohen\n",
    "print('Cohens Kappa = {}'.format(agreementStats.kappa()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B - Alternative Data Source Examples\n",
    "Examples of loading data from:\n",
    "1. Oracle\n",
    "1. Recap\n",
    "\n",
    "This is real code from our efforts to derive an automated SOFA score.  Please use these as starting points for your projects!  Refer to README for additional python packages that must be installed for obtaining data from these sources.  The README will also point you to RedCap Project schemas and SQL DDL that allows you to create the underlying data sources in your ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from Oracle Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "These additional modules are required for loading data From Oracle\n",
    "Also note that Python relies on the underlying operating system to make database connections\n",
    "using the ODBC protocol\n",
    "For each database engine Oracle, mySQL, Postgres, etc.. you wish to connect to there will be some\n",
    "OS level dependencies to install, typically some db engine specific client libraries\n",
    "This approach will work for any database you can create a connection to\n",
    "'''\n",
    "\n",
    "import sqlalchemy\n",
    "import cx_Oracle\n",
    "# used to mask secrets that might be entered into the notebook\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection\n",
    "# Here we set up a connection to that schema, prompting for password so it is not saved in the notebook!\n",
    "dbUser = input(\"Enter database Username: \")\n",
    "dbPass = getpass.getpass(\"Enter database Password: \")\n",
    "dbUrl = input(\"Enter database URL: \")\n",
    "\n",
    "dbEngineURL = 'oracle+cx_oracle://{}:{}@{}'.format(dbUser, dbPass, dbUrl)\n",
    "dbEngine = sqlalchemy.create_engine(dbEngineURL, pool_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"You will need to edit this sql to match the specifics of your data table!\"\"\"\n",
    "computedSQL = \"\"\"\n",
    "    select\n",
    "        PAT_ENC_CSN_ID,\n",
    "        TSOFA0TO24COAG,\n",
    "        TSOFA0TO24CV,\n",
    "        TSOFA0TO24GCS,\n",
    "        TSOFA0TO24HEPATIC,\n",
    "        ESOFA0TO24RENAL,\n",
    "        TSOFA0TO24RESP\n",
    "    from\n",
    "        pydata_tsofa\"\"\"\n",
    "\n",
    "# dictionary used to map sql column names to conformed column names needed for comparison\n",
    "computedSQLRenameDict = {\n",
    "    'tsofa0to24coag':'coag',\n",
    "    'tsofa0to24cv':'cv',\n",
    "    'tsofa0to24gcs':'cns',\n",
    "    'tsofa0to24hepatic':'hepatic',\n",
    "    'esofa0to24renal':'renal',\n",
    "    'tsofa0to24resp':'resp'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell:\n",
    "1. Reads data from the database using the SQL query defined above\n",
    "2. Renames the columns to the standard names used in the comparison\n",
    "3. Sets the index\n",
    "4. Sorts the index\n",
    "5. De-dupes the data.  Should not be necessary.  If you notice a change here, review your data source!\n",
    "'''\n",
    "computedDf = pd.read_sql_query(computedSQL, dbEngine)\n",
    "computedDf.rename(columns=computedSQLRenameDict, inplace=True)\n",
    "computedDf.set_index(['pat_enc_csn_id'], inplace=True)\n",
    "computedDf.sort_index(inplace=True)\n",
    "\n",
    "# de-dupe, if necessary\n",
    "print('Before de-duping the index is unique: {}'.format(computedDf.index.is_unique))\n",
    "computedDf = (computedDf.reset_index()\n",
    "                    .drop_duplicates()\n",
    "                    .set_index('pat_enc_csn_id'))\n",
    "print('After de-duping the index is unique: {}'.format(computedDf.index.is_unique))\n",
    "\n",
    "computedDf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data From Redcap\n",
    "In our original use case this is where a team of 3 physicians has entered the manually abstracted SOFA scores.  Using the RedCap API, the scores were extracted and converted into a dataframe before feeding into the rest of the data comparison workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules used to obtain data from Redcap\n",
    "import pycurl\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlencode\n",
    "import json\n",
    "# used to mask secrets that might be entered into the notebook\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redCapApiToken = getpass.getpass(\"Please enter your RedCap API Token:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell:\n",
    "1. Reads data from the RedCap using the API\n",
    "2. Renames the columns to the standard names used in the comparison\n",
    "3. Sets the index\n",
    "4. Sorts the index\n",
    "byteBuf = BytesIO()\n",
    "'''\n",
    "\n",
    "# Set up post data\n",
    "data = {\n",
    "    'token': redCapApiToken,\n",
    "    'content': 'record',\n",
    "    'format': 'json',\n",
    "    'type': 'flat',\n",
    "    'events[0]': 'cr1_arm_1',\n",
    "    'rawOrLabel': 'raw',\n",
    "    'rawOrLabelHeaders': 'raw',\n",
    "    'exportCheckboxLabel': 'false',\n",
    "    'exportSurveyFields': 'false',\n",
    "    'exportDataAccessGroups': 'false',\n",
    "    'returnFormat': 'json'\n",
    "}\n",
    "postfields = urlencode(data)\n",
    "\n",
    "# create CURL object to perform the POST request\n",
    "ch = pycurl.Curl()\n",
    "ch.setopt(ch.URL, 'https://redcap.ucdmc.ucdavis.edu/redcap/api/')    \n",
    "ch.setopt(ch.POSTFIELDS, postfields)\n",
    "ch.setopt(ch.WRITEDATA, byteBuf)\n",
    "ch.perform()\n",
    "ch.close()\n",
    "\n",
    "# # DECODE the byte buffer return\n",
    "body = byteBuf.getvalue()\n",
    "bodyStr = body.decode('utf-8')\n",
    "bodyJSON = json.loads(bodyStr)\n",
    "goldstandardDf = pd.DataFrame(bodyJSON)\n",
    "# turn pat_enc_csn colum to integer column\n",
    "goldstandardDf['pat_enc_csn_id'] = goldstandardDf['pat_enc_csn_id'].astype('int64')\n",
    "# Set csn as index\n",
    "goldstandardDf.set_index('pat_enc_csn_id', inplace=True)\n",
    "goldstandardDf.sort_index(inplace=True)\n",
    "# map column names to standards for comparison\n",
    "renameDict = {'tsofa0to24cns':'cns',\n",
    "'tsofa0to24coag':'coag',\n",
    "'tsofa0to24cv':'cv',\n",
    "'tsofa0to24hepatic':'hepatic',\n",
    "'tsofa0to24renal':'renal',\n",
    "'tsofa0to24resp':'resp'}\n",
    "\n",
    "goldstandardDf = goldstandardDf.rename(columns=renameDict)\n",
    "# make sofa score cols floats!\n",
    "numCols = ['coag','cv','cns','hepatic','renal','resp']\n",
    "goldstandardDf[numCols] = goldstandardDf[numCols].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "\n",
    "goldstandardDf.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucd-ri-datacompare",
   "language": "python",
   "name": "ucd-ri-datacompare"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
